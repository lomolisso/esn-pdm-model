{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESN-PdM Framework ML models\n",
    "This notebook contains the implementation of the ESN-PdM framework's machine learning models. These models are developed using the `tensorflow` library and trained with a dataset specifically generated for this project. The data is located in the `dataset` folder of this repository and consists of several files, each corresponding to one of the four classes the models are trained to predict. The data was generated using a BMI270 IMU and an ESP32, hence the features include the raw accelerometer and gyroscope readings from the IMU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, Constants and Converters\n",
    "First, we import the necessary libraries and define some constants and converters that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Disable logs except for errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "INPUT_DATA_PATH = 'datos/'\n",
    "\n",
    "LABELS = {\n",
    "    \"Good\": 0,\n",
    "    \"Acceptable\": 1,\n",
    "    \"Unacceptable\": 2,\n",
    "    \"Bad\": 3\n",
    "}\n",
    "\n",
    "# --- Accelerometer Constants ---\n",
    "G_MS2 = 9.80665\n",
    "MAX_INT_VALUE_SENSOR = 32768.0\n",
    "ACC_RAW_TO_MS2 = (G_MS2 / MAX_INT_VALUE_SENSOR)\n",
    "SENSOR_ACC_RANGE = 2 # 8g\n",
    "\n",
    "# --- Gyroscope Constants ---\n",
    "SENSOR_GYR_RANGE = 250.0\n",
    "PI = 3.14159265359\n",
    "GYR_RAW_TO_RADS = (PI / 180.0) / MAX_INT_VALUE_SENSOR\n",
    "\n",
    "# --- Converters ---\n",
    "convert_raw_acc_to_ms2 = lambda raw: (pow(2, SENSOR_ACC_RANGE + 1) * ACC_RAW_TO_MS2) * raw\n",
    "convert_raw_gyr_to_rads = lambda raw: SENSOR_GYR_RANGE * GYR_RAW_TO_RADS * raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "Next, we load the data and preprocess it. The data is loaded from the files in the `dataset` folder and preprocessed to be used in the models. The preprocessing steps include cleaning null values, converting the raw data to the appropriate format and normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INPUT_DATA_PATH + 'column_names.json') as f:\n",
    "    column_names = json.load(f)\n",
    "\n",
    "labeled_data_frames = {l: [] for l in LABELS.values()}\n",
    "for filename in os.listdir(INPUT_DATA_PATH):\n",
    "    for label in LABELS.values():\n",
    "        if filename.startswith(str(label)) and filename.endswith('.csv'):\n",
    "            df = pd.read_csv(INPUT_DATA_PATH + filename, names=column_names, sep=';')\n",
    "            \n",
    "            # Convert the timestamp column to a datetime object\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            \n",
    "            # Create a mask for rows where acc_x, acc_y, and acc_z are all 0\n",
    "            mask_acc_zero = (df['acc_x'] == 0) & (df['acc_y'] == 0) & (df['acc_z'] == 0)\n",
    "\n",
    "            # Create a mask for rows where gyro_x, gyro_y, and gyro_z are all 0\n",
    "            mask_gyro_zero = (df['gyro_x'] == 0) & (df['gyro_y'] == 0) & (df['gyro_z'] == 0)\n",
    "\n",
    "            # Combine the masks to identify rows where either condition is true\n",
    "            mask_either_zero = mask_acc_zero | mask_gyro_zero\n",
    "\n",
    "            # Filter out the rows from the DataFrame\n",
    "            df = df[~mask_either_zero]\n",
    "\n",
    "            # Convert the raw accelerometer data to m/s^2\n",
    "            df['acc_x'] = df['acc_x'].apply(convert_raw_acc_to_ms2).astype(\"float32\")\n",
    "            df['acc_y'] = df['acc_y'].apply(convert_raw_acc_to_ms2).astype(\"float32\")\n",
    "            df['acc_z'] = df['acc_z'].apply(convert_raw_acc_to_ms2).astype(\"float32\")\n",
    "\n",
    "            # Convert the raw gyroscope data to rad/s\n",
    "            df['gyro_x'] = df['gyro_x'].apply(convert_raw_gyr_to_rads).astype(\"float32\")\n",
    "            df['gyro_y'] = df['gyro_y'].apply(convert_raw_gyr_to_rads).astype(\"float32\")\n",
    "            df['gyro_z'] = df['gyro_z'].apply(convert_raw_gyr_to_rads).astype(\"float32\")\n",
    "            \n",
    "            labeled_data_frames[label].append(df)\n",
    "\n",
    "dataframes = {label: pd.concat(data_frames) for label, data_frames in labeled_data_frames.items()}\n",
    "\n",
    "# Make a copy of all the dataframes, add a label column, and concatenate them into a single dataframe\n",
    "labeled_data = pd.concat([df.assign(label=label) for label, df in dataframes.items()])\n",
    "labeled_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print info and overall stats about the dataset\n",
    "print(labeled_data.info())\n",
    "print(labeled_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Generation and Data Splitting\n",
    "After preprocessing the data, we generate sequences that will be used for training the models. These sequences are created by filling a buffer with `SEG_LENGTH` contiguous readings. Once the sequences are generated, we split the data into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 25\n",
    "\n",
    "# crop dataframes to a number of rows that is a multiple of SEQ_LENGTH\n",
    "for label, data in dataframes.items():\n",
    "    dataframes[label] = data.iloc[:len(data) - len(data) % SEQ_LENGTH]\n",
    "\n",
    "# Crear secuencias y etiquetas\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Create sequences and labels\n",
    "for label, data in dataframes.items():\n",
    "    for i in range(0, len(data), SEQ_LENGTH):\n",
    "        seq = data.iloc[i:i + SEQ_LENGTH]\n",
    "        sequences.append(seq[['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']].values)\n",
    "        labels.append(label)\n",
    "\n",
    "sequences, labels = np.array(sequences), np.array(labels)\n",
    "\n",
    "# Print the number of sequences to be fed into the model\n",
    "print(f\"Sequences: {sequences.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(sequences, labels, test_size=0.4, random_state=random.randint(0, 100))\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random.randint(0, 100))\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(LABELS))\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=len(LABELS))\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "We define the model architecture, which consists of parallel `LSTM` and `Conv1D` layers followed by fully connected layers. The RNN layer operates on the sequence in the temporal domain, while the CNN layer operates on the sequence in the frequency domain by transforming the signal through `FFT`. This structure aims to capture both the temporal and spatial features of the data. The model is compiled using the Adam optimizer and the categorical cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_model():\n",
    "    # Input layer\n",
    "    inputs = tf.keras.layers.Input(shape=(SEQ_LENGTH, 6))\n",
    "\n",
    "    # Convolutional layers\n",
    "    conv1 = tf.keras.layers.SeparableConv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv2 = tf.keras.layers.SeparableConv1D(filters=64, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    pool = tf.keras.layers.MaxPooling1D()(conv2)\n",
    "    pool_flat = tf.keras.layers.Flatten()(pool)\n",
    "\n",
    "    # LSTM\n",
    "    lstm = tf.keras.layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    lstm_flat = tf.keras.layers.Flatten()(lstm)\n",
    "\n",
    "    # Concatenation of Conv and LSTM paths\n",
    "    concat = tf.keras.layers.Concatenate()([pool_flat, lstm_flat])\n",
    "\n",
    "    # Dense layers\n",
    "    dense1 = tf.keras.layers.Dense(128, activation='relu')(concat)\n",
    "    dense2 = tf.keras.layers.Dense(64, activation='relu')(dense1)\n",
    "    dense3 = tf.keras.layers.Dense(32, activation='relu')(dense2)\n",
    "    outputs = tf.keras.layers.Dense(len(LABELS), activation='softmax')(dense3)\n",
    "\n",
    "    # Model creation\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Model\n",
    "The model to be deployed on the cloud layer of the ESN-PdM framework is trained using the training and validation sets. The model is trained for `50` epochs using mini-batches of size `32`. To avoid overfitting, we use early stopping with a patience of `5` epochs, i.e., if the validation loss does not improve for `5` consecutive epochs, the training is stopped. The model is saved to a file named `cloud_model.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training constants\n",
    "CLOUD_MODEL_TRAIN_EPOCHS = 15\n",
    "MAX_EPOCHS_WITHOUT_IMPROVEMENT = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create the model\n",
    "cloud_model = create_model()\n",
    "\n",
    "# Compile the model\n",
    "cloud_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=MAX_EPOCHS_WITHOUT_IMPROVEMENT, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "cloud_model.fit(X_train, y_train, epochs=CLOUD_MODEL_TRAIN_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "cloud_y_pred = cloud_model.predict(X_test)\n",
    "cloud_y_pred_classes = np.argmax(cloud_y_pred, axis=1)\n",
    "cloud_model_accuracy = np.mean(cloud_y_pred_classes == y_true)\n",
    "\n",
    "# Save the model\n",
    "cloud_model.save('cloud_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_true, cloud_y_pred_classes, target_names=LABELS.keys()))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(confusion_matrix(y_true, cloud_y_pred_classes))\n",
    "\n",
    "# Print model accuracy\n",
    "print(f\"Model accuracy: {cloud_model_accuracy}\")\n",
    "\n",
    "# Print model summary\n",
    "cloud_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gateway Model\n",
    "Unlike a cloud server, a gateway, such as the Raspberry Pi 4 used in the ESN-PdM framework, has limited computational resources. Therefore, the model deployed on the gateway layer needs to be lightweight yet accurate. To achieve this, we **fine-tune the pre-trained cloud model using pruning**. Pruning is a technique that removes less important weights from the model, reducing its size and computational complexity. During the fine-tuning process, the pruning sparsity is gradually increased from `0.50` to `0.80`. Pruning sparsity is the ratio of the number of weights zeroed out to the total number of weights in the model. In other words, after pruning, the resulting model will have 80% of its weights zeroed out.\n",
    "\n",
    "The pruned model is then converted to a TensorFlow Lite model, optimized for deployment on edge devices. Among these optimizations is **post-training quantization**, which further reduces the model's size and computational complexity by converting its parameters from 32-bit floating-point numbers to 8-bit integers. Specifically, the type of quantization used for the gateway model is **dynamic range quantization**. This type of quantization only affects the weights of the model, leaving the activations in floating-point format.\n",
    "\n",
    "The final model is saved to a file named `gateway_model.tflite`. Note that applying a standard compression algorithm, such as gzip, is necessary to fully realize the compression benefits of pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "GATEWAY_FINE_TUNE_EPOCHS = 3\n",
    "FINE_TUNE_EPOCHS_WITHOUT_IMPROVEMENT = 1\n",
    "\n",
    "num_sequences = X_train.shape[0]\n",
    "end_step = np.ceil(num_sequences / BATCH_SIZE).astype(np.int32) * GATEWAY_FINE_TUNE_EPOCHS\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.50,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(\n",
    "      monitor='val_accuracy',\n",
    "      patience=FINE_TUNE_EPOCHS_WITHOUT_IMPROVEMENT,\n",
    "      restore_best_weights=False\n",
    "  ),\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "]\n",
    "\n",
    "# Load the cloud model and fine-tune it with pruning\n",
    "_loaded_cloud_model = tf.keras.models.load_model('cloud_model.keras')\n",
    "gateway_model = prune_low_magnitude(_loaded_cloud_model, **pruning_params)\n",
    "\n",
    "# Compile the model\n",
    "gateway_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "gateway_model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=GATEWAY_FINE_TUNE_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "gateway_y_pred = gateway_model.predict(X_test)\n",
    "gateway_y_pred_classes = np.argmax(gateway_y_pred, axis=1)\n",
    "gateway_model_accuracy = np.sum(y_true == gateway_y_pred_classes) / len(y_true)\n",
    "\n",
    "# Print classification report\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_true, gateway_y_pred_classes, target_names=LABELS.keys()))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(confusion_matrix(y_true, gateway_y_pred_classes))\n",
    "\n",
    "# Print accuracy before tflite conversion\n",
    "print(f\"Gateway model accuracy: {gateway_model_accuracy}\")\n",
    "\n",
    "# Print the gateway model summary\n",
    "gateway_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pruning wrappers from the pruned model\n",
    "gateway_model = tfmot.sparsity.keras.strip_pruning(gateway_model)\n",
    "\n",
    "# Make a Tensorflow Lite version of the model and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(gateway_model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "\n",
    "# Dynamic range quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  \n",
    "\n",
    "bytes_gateway_model = converter.convert()\n",
    "with open('gateway_model.tflite', 'wb') as f:\n",
    "    f.write(bytes_gateway_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Model\n",
    "Finally, we train a model to be deployed on the ESP32 microcontroller. Given the minimal computational resources available on the ESP32, the model needs to be extremely lightweight. Therefore, we simplify the model architecture by employing only `Conv1D` layers and reducing the number of neurons in the fully connected layers. The optimizations for the sensor model go beyond those used for the gateway model. In addition to **pruning**, the model is quantized using **full integer quantization**. This type of quantization converts both the weights and activations of the model from 32-bit floating-point numbers to 8-bit integers. Full integer quantization is more aggressive than dynamic range quantization, but it requires calibrating the quantization parameters using a representative dataset. The calibration dataset is generated by sampling a subset of the training data. It is important to note that LSTM layers are not used in the sensor model because they are computationally expensive and are not included in TensorFlow Lite's `TFLITE_BUILTINS_INT8` operation set, which is required for full integer quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_MODEL_TRAIN_EPOCHS = 15\n",
    "SENSOR_FINE_TUNE_EPOCHS = 5\n",
    "\n",
    "# --- Sensor Model Training ---\n",
    "print(\"Training sensor model...\")\n",
    "sensor_model = tf.keras.Sequential([\n",
    "    # Input layer\n",
    "    tf.keras.layers.Input(shape=(SEQ_LENGTH, 6)),\n",
    "    \n",
    "    # Convolutional layers\n",
    "    tf.keras.layers.SeparableConv1D(filters=16, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # Dense layers\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])\n",
    "\n",
    "callbakcs = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=MAX_EPOCHS_WITHOUT_IMPROVEMENT, restore_best_weights=True)\n",
    "]\n",
    "sensor_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "sensor_model.fit(X_train, y_train, epochs=SENSOR_MODEL_TRAIN_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "\n",
    "# --- Sensor Model Fine-Tuning ---\n",
    "print(\"Fine-tuning sensor model...\")\n",
    "num_sequences = X_train.shape[0]\n",
    "end_step = np.ceil(num_sequences / BATCH_SIZE).astype(np.int32) * SENSOR_FINE_TUNE_EPOCHS\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=0.50,\n",
    "        begin_step=0,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "\n",
    "fine_tuning_callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=MAX_EPOCHS_WITHOUT_IMPROVEMENT, restore_best_weights=False)\n",
    "]\n",
    "\n",
    "# Create a pruned model\n",
    "sensor_model = prune_low_magnitude(sensor_model, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "sensor_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune with pruning\n",
    "sensor_model.fit(X_train, y_train, epochs=SENSOR_FINE_TUNE_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), callbacks=fine_tuning_callbacks)\n",
    "\n",
    "# Evaluate the model\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "sensor_y_pred = sensor_model.predict(X_test)\n",
    "sensor_y_pred_classes = np.argmax(sensor_y_pred, axis=1)\n",
    "sensor_model_accuracy = np.sum(y_true == sensor_y_pred_classes) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"--- Classification Report ---\")\n",
    "print(classification_report(y_true, sensor_y_pred_classes, target_names=LABELS.keys()))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(confusion_matrix(y_true, sensor_y_pred_classes))\n",
    "\n",
    "# Print accuracy before tflite conversion\n",
    "print(f\"Sensor model accuracy: {sensor_model_accuracy}\")\n",
    "\n",
    "# Print the sensor model summary\n",
    "sensor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pruning wrappers from the pruned model\n",
    "sensor_model = tfmot.sparsity.keras.strip_pruning(sensor_model)\n",
    "\n",
    "# Representative dataset for quantization\n",
    "def representative_dataset():\n",
    "    for i in range(0, X_train.shape[0], BATCH_SIZE):\n",
    "        yield [X_train[i:i + BATCH_SIZE]]\n",
    "\n",
    "# Make a Tensorflow Lite version of the model and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(sensor_model)\n",
    "\n",
    "print(\"Converting sensor model to TFLite...\")\n",
    "\n",
    "# Full integer quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "bytes_sensor_model = converter.convert()\n",
    "with open('sensor_model.tflite', 'wb') as f:\n",
    "    f.write(bytes_sensor_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "Below me compare the performance of the cloud, gateway, and sensor models in terms of accuracy and size. The accuracy is evaluated using the test set, while the size is measured in terms of the compressed model file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size comparison\n",
    "def get_gzipped_model_size(file):\n",
    "  import os, zipfile, tempfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "  return os.path.getsize(zipped_file)\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    if size_bytes < 1024:\n",
    "        return f\"{size_bytes:.2f} bytes\"\n",
    "    elif size_bytes < 1024 ** 2:\n",
    "        return f\"{size_bytes / 1024:.2f} KB\"\n",
    "    else:\n",
    "        return f\"{size_bytes / 1024 ** 2:.2f} MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison\n",
    "def _quantize_model(input_data, input_details):\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    input_data = input_data / input_scale + input_zero_point\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def evaluate_tflite_model(tflite_model_path, X_test, y_test):\n",
    "    # Load the TFLite model and allocate tensors\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensor details\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        input_data = _quantize_model(X_test[i], input_details) if input_details['dtype'] == np.uint8 else X_test[i]\n",
    "\n",
    "        input_data = np.expand_dims(input_data, axis=0).astype(input_details['dtype'])   \n",
    "        interpreter.set_tensor(input_details['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details['index'])\n",
    "        \n",
    "        predicted_label = np.argmax(output_data)\n",
    "\n",
    "\n",
    "        true_label = np.argmax(y_test[i])\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(sequences, labels, test_size=0.4, random_state=random.randint(0, 100))\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random.randint(0, 100))\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(LABELS))\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=len(LABELS))\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=len(LABELS))\n",
    "\n",
    "cloud_size = get_gzipped_model_size(\"cloud_model.keras\")\n",
    "gateway_size = get_gzipped_model_size(\"gateway_model.tflite\")\n",
    "sensor_size = get_gzipped_model_size(\"sensor_model.tflite\")\n",
    "\n",
    "tflite_gateway_model_accuracy = evaluate_tflite_model('gateway_model.tflite', X_test, y_test)\n",
    "tflite_sensor_model_accuracy = evaluate_tflite_model('sensor_model.tflite', X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model sizes\n",
    "print(\"\\n---------------------- Model Sizes ----------------------\")\n",
    "print(\"Size of cloud Keras model: %s\" % format_size(os.path.getsize(\"cloud_model.keras\")))\n",
    "print(\"Size of gateway Tflite model: %s\" % format_size(os.path.getsize(\"gateway_model.tflite\")))\n",
    "print(\"Size of sensor Tflite model: %s\" % format_size(os.path.getsize(\"sensor_model.tflite\")))\n",
    "\n",
    "print(\"\\n---------------------- Gzipped Model Sizes ----------------------\")\n",
    "print(\"Size of gzipped cloud Keras model: %s\" % format_size(cloud_size))\n",
    "print(\"Size of gzipped gateway Tflite model: %s\" % format_size(gateway_size))\n",
    "print(\"Size of gzipped sensor Tflite model: %s\" % format_size(sensor_size))\n",
    "\n",
    "# Print how much smaller the gateway model is\n",
    "print(\"\\n---------------------- Model Size Comparison ----------------------\")\n",
    "print(\"Gateway model is %.2f times smaller than the cloud model\" % (get_gzipped_model_size(\"cloud_model.keras\") / get_gzipped_model_size(\"gateway_model.tflite\")))\n",
    "print(\"Sensor model is %.2f times smaller than the cloud model\" % (get_gzipped_model_size(\"cloud_model.keras\") / get_gzipped_model_size(\"sensor_model.tflite\")))\n",
    "print(\"Sensor model is %.2f times smaller than the gateway model\" % (get_gzipped_model_size(\"gateway_model.tflite\") / get_gzipped_model_size(\"sensor_model.tflite\")))\n",
    "\n",
    "# Print the model accuracies\n",
    "print(\"\\n---------------------- Model Accuracies ----------------------\")\n",
    "print(f\"Cloud model accuracy: {cloud_model_accuracy * 100:.2f}% [Tensorflow Keras]\")\n",
    "print(f\"Gateway model accuracy: {tflite_gateway_model_accuracy * 100:.2f}% [TensorFlow Lite]\")\n",
    "print(f\"Sensor model accuracy: {tflite_sensor_model_accuracy * 100:.2f}% [TensorFlow Lite Micro]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
